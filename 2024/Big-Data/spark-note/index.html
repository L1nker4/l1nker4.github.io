

<!DOCTYPE html>
<html lang="zh-CN" >



<head>
  <meta charset="UTF-8">

  <link rel="apple-touch-icon" sizes="76x76" href="/favicon.ico">
  <link rel="icon" href="/favicon.ico">
  

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
    <meta http-equiv="Content-Security-Policy" content="upgrade-insecure-requests">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Lin Wang">
  <meta name="keywords" content="l1nker4">
  
    <meta name="description" content="简介Apache Spark是一个分布式计算系统，具备以下特点：  使用RDD(Resilient Distributed Datasets)，提供了比MapReduce更为丰富的模型 基于内存计算，性能比MapReduce模型高 集成离线计算、实时计算、机器学习、图计算等模块  核心模块：  Spark Core：提供Spark核心功能，是SQL、Streaming等模块实现的基础。 Spark">
<meta property="og:type" content="article">
<meta property="og:title" content="Spark入门">
<meta property="og:url" content="https://l1n.wang/2024/Big-Data/spark-note/index.html">
<meta property="og:site_name" content="l1nker4&#39;s Blog">
<meta property="og:description" content="简介Apache Spark是一个分布式计算系统，具备以下特点：  使用RDD(Resilient Distributed Datasets)，提供了比MapReduce更为丰富的模型 基于内存计算，性能比MapReduce模型高 集成离线计算、实时计算、机器学习、图计算等模块  核心模块：  Spark Core：提供Spark核心功能，是SQL、Streaming等模块实现的基础。 Spark">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://blog-1251613845.cos.ap-shanghai.myqcloud.com/20240729135117.png">
<meta property="og:image" content="https://blog-1251613845.cos.ap-shanghai.myqcloud.com/20240912195907.png">
<meta property="og:image" content="https://blog-1251613845.cos.ap-shanghai.myqcloud.com/20240731141319.png">
<meta property="article:published_time" content="2024-07-12T05:20:18.000Z">
<meta property="article:modified_time" content="2024-12-07T10:17:54.106Z">
<meta property="article:author" content="Lin Wang">
<meta property="article:tag" content="l1nker4">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://blog-1251613845.cos.ap-shanghai.myqcloud.com/20240729135117.png">
  
  
  
  <title>Spark入门 - l1nker4&#39;s Blog</title>

  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@4/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hint.css@2/hint.min.css" />

  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1749284_5i9bdhy70f8.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  



  
<link rel="stylesheet" href="/css/custom.css">



  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"l1n.wang","root":"/","version":"1.9.8","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"right","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":true,"follow_dnt":true,"baidu":"612bbac3c11e86af5c472b8381f09fa7","google":null,"tencent":{"sid":null,"cid":null},"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false},"umami":{"src":null,"website_id":null,"domains":null,"start_time":"2024-01-01T00:00:00.000Z","token":null,"api_server":null},"gtag":null,"woyaola":null,"cnzz":null},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  

  
    <!-- Baidu Analytics -->
    <script async>
      if (!Fluid.ctx.dnt) {
        var _hmt = _hmt || [];
        (function() {
          var hm = document.createElement("script");
          hm.src = "https://hm.baidu.com/hm.js?612bbac3c11e86af5c472b8381f09fa7";
          var s = document.getElementsByTagName("script")[0];
          s.parentNode.insertBefore(hm, s);
        })();
      }
    </script>
  

  

  

  

  



  
<meta name="generator" content="Hexo 7.3.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>l1nker4&#39;s Blog</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/" target="_self">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/" target="_self">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/links/" target="_self">
                <i class="iconfont icon-link-fill"></i>
                <span>友链</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('https://blog-1251613845.cos.ap-shanghai.myqcloud.com/bg/bg1.jpg') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="Spark入门"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2024-07-12 13:20" pubdate>
          2024年7月12日 下午
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          <!-- compatible with older versions-->
          3.1k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          <!-- compatible with older versions-->
          27 分钟
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">Spark入门</h1>
            
            
              <div class="markdown-body">
                
                <h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>Apache Spark是一个分布式计算系统，具备以下特点：</p>
<ul>
<li>使用RDD(Resilient Distributed Datasets)，提供了比MapReduce更为丰富的模型</li>
<li>基于内存计算，性能比MapReduce模型高</li>
<li>集成离线计算、实时计算、机器学习、图计算等模块</li>
</ul>
<p>核心模块：</p>
<ul>
<li>Spark Core：提供Spark核心功能，是SQL、Streaming等模块实现的基础。</li>
<li>Spark SQL：提供SQL进行数据查询的组件</li>
<li>Spark Streaming：提供流式计算的组件</li>
<li>MLlib：Spark平台的机器学习算法库</li>
<li>GraphX：面向图计算的组件</li>
</ul>
<p>Spark名词解释：</p>
<table>
<thead>
<tr>
<th><strong>名称</strong></th>
<th><strong>含义</strong></th>
</tr>
</thead>
<tbody><tr>
<td>Application</td>
<td>指用户提交的 Spark 应用程序</td>
</tr>
<tr>
<td>Job</td>
<td>指 Spark 作业，是 Application 的子集，由行动算子（action）触发</td>
</tr>
<tr>
<td>Stage</td>
<td>指 Spark 阶段，是 Job 的子集，以 RDD 的宽依赖为界</td>
</tr>
<tr>
<td>Task</td>
<td>指 Spark 任务，是 Stage 的子集，Spark 中最基本的任务执行单元，对应单个线程，会被封装成 TaskDescription 对象提交到 Executor 的线程池中执行</td>
</tr>
<tr>
<td>Driver</td>
<td>运行用户程序<code>main()</code>方法并创建SparkContext的实例</td>
</tr>
<tr>
<td>Cluster Manager</td>
<td>集群管理器，例如Yarn、Mesos、Kubernetes等</td>
</tr>
</tbody></table>
<p><img src="https://blog-1251613845.cos.ap-shanghai.myqcloud.com/20240729135117.png" srcset="/img/loading.gif" lazyload alt="image.png"></p>
<p>Spark运行过程：</p>
<ol>
<li>Driver执行用户程序的main方法，并创建SparkContext，与Cluster Manager建立连接。</li>
<li>Cluster Manager为用户程序分配计算资源，返回可使用的Executor列表。</li>
<li>获取Executor资源后，Spark会将用户程序代码以及依赖包，发送给Executor</li>
<li>SparkContext发送task到Executor，由executor执行计算任务。</li>
</ol>
<h3 id="Demo"><a href="#Demo" class="headerlink" title="Demo"></a>Demo</h3><div class="code-wrapper"><pre><code class="hljs xml"><span class="hljs-tag">&lt;<span class="hljs-name">dependency</span>&gt;</span>  
    <span class="hljs-tag">&lt;<span class="hljs-name">groupId</span>&gt;</span>org.apache.spark<span class="hljs-tag">&lt;/<span class="hljs-name">groupId</span>&gt;</span>  
    <span class="hljs-tag">&lt;<span class="hljs-name">artifactId</span>&gt;</span>spark-core_2.12<span class="hljs-tag">&lt;/<span class="hljs-name">artifactId</span>&gt;</span>  
    <span class="hljs-tag">&lt;<span class="hljs-name">version</span>&gt;</span>3.1.2<span class="hljs-tag">&lt;/<span class="hljs-name">version</span>&gt;</span>  
    <span class="hljs-tag">&lt;<span class="hljs-name">scope</span>&gt;</span>provided<span class="hljs-tag">&lt;/<span class="hljs-name">scope</span>&gt;</span>  
<span class="hljs-tag">&lt;/<span class="hljs-name">dependency</span>&gt;</span></code></pre></div>



<div class="code-wrapper"><pre><code class="hljs java"><span class="hljs-keyword">import</span> org.apache.spark.SparkConf;  
<span class="hljs-keyword">import</span> org.apache.spark.api.java.JavaPairRDD;  
<span class="hljs-keyword">import</span> org.apache.spark.api.java.JavaRDD;  
<span class="hljs-keyword">import</span> org.apache.spark.api.java.JavaSparkContext;  
<span class="hljs-keyword">import</span> scala.Tuple2;  
  
<span class="hljs-keyword">import</span> java.util.Arrays;  
  
<span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">WordCountDemo</span> &#123;  
  
    <span class="hljs-keyword">private</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">wordCount</span><span class="hljs-params">(String fileName)</span> &#123;  
  
        <span class="hljs-type">SparkConf</span> <span class="hljs-variable">sparkConf</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">SparkConf</span>().setMaster(<span class="hljs-string">&quot;local&quot;</span>).setAppName(<span class="hljs-string">&quot;JD Word Counter&quot;</span>);  
  
        <span class="hljs-type">JavaSparkContext</span> <span class="hljs-variable">sparkContext</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">JavaSparkContext</span>(sparkConf);  
  
        JavaRDD&lt;String&gt; inputFile = sparkContext.textFile(fileName);  
  
        JavaRDD&lt;String&gt; wordsFromFile = inputFile.flatMap(content -&gt; Arrays.asList(content.split(<span class="hljs-string">&quot; &quot;</span>)).iterator());  
  
        <span class="hljs-type">JavaPairRDD</span> <span class="hljs-variable">countData</span> <span class="hljs-operator">=</span> wordsFromFile.mapToPair(t -&gt; <span class="hljs-keyword">new</span> <span class="hljs-title class_">Tuple2</span>(t, <span class="hljs-number">1</span>)).reduceByKey((x, y) -&gt; (<span class="hljs-type">int</span>) x + (<span class="hljs-type">int</span>) y);  
  
        countData.saveAsTextFile(<span class="hljs-string">&quot;CountData&quot;</span>);  
    &#125;  
  
    <span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">main</span><span class="hljs-params">(String[] args)</span> &#123;  
  
        <span class="hljs-keyword">if</span> (args.length == <span class="hljs-number">0</span>) &#123;  
            System.out.println(<span class="hljs-string">&quot;No files provided.&quot;</span>);  
            System.exit(<span class="hljs-number">0</span>);  
        &#125;  
  
        wordCount(args[<span class="hljs-number">0</span>]);  
    &#125;  
&#125;</code></pre></div>

<p>提交应用：</p>
<div class="code-wrapper"><pre><code class="hljs shell">bin/spark-submit \
--class com.github.l1nker4.spark.WordCountDemo \
--master local \
spark-demo-1.0-SNAPSHOT.jar data/word.txt</code></pre></div>


<p>也可以通过spark-shell的方式，提交应用：</p>
<div class="code-wrapper"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash">需要配置JAVA_HOME</span>
bin/spark-shell
<span class="hljs-meta prompt_"></span>
<span class="hljs-meta prompt_"># </span><span class="language-bash">data/word.txt提前创建</span>
<span class="hljs-meta prompt_">scala&gt; </span><span class="language-bash">sc.textFile(<span class="hljs-string">&quot;data/word.txt&quot;</span>).flatMap(_.<span class="hljs-built_in">split</span>(<span class="hljs-string">&quot;\n&quot;</span>)).map((_,<span class="hljs-number">1</span>)).reduceByKey(_+_).collect</span>
res6: Array[(String, Int)] = Array((zhangsan,2), (wangwu,1), (lisi,1))
</code></pre></div>


<h2 id="部署方式"><a href="#部署方式" class="headerlink" title="部署方式"></a>部署方式</h2><ul>
<li>local模式：单机多线程模拟Spark分布式计算，通常用于开发调试。</li>
<li>Standalone模式：以master-slave模式部署Spark集群。<ul>
<li>master参数指定为master节点地址，例如：<code>spark://master:7077</code></li>
</ul>
</li>
<li>YARN模式：集群计算资源调度由YARN管理，无需部署启动Spark集群。<ul>
<li>master参数指定为yarn</li>
</ul>
</li>
</ul>
<h3 id="local模式"><a href="#local模式" class="headerlink" title="local模式"></a>local模式</h3><p>Web UI：<a target="_blank" rel="noopener" href="http://localhost:4040/jobs/">http://localhost:4040/jobs/</a></p>
<p>提交应用：</p>
<div class="code-wrapper"><pre><code class="hljs shell">bin/spark-submit \
--class org.apache.spark.examples.SparkPi \
--master local[2] \
./examples/jars/spark-examples_2.12-3.1.2.jar \
10 # 入口参数</code></pre></div>

<p>参数说明：</p>
<ul>
<li>本地模式下，master取值如下：<ul>
<li><code>local</code>：只启动一个Executor</li>
<li><code>local[N]</code>：启动N个Executor</li>
<li><code>local[*]</code>：启动CPU核数相同的Executor</li>
</ul>
</li>
</ul>
<h2 id="RDD"><a href="#RDD" class="headerlink" title="RDD"></a>RDD</h2><p>RDD(Resilient Distributed Datasets)代表一个不可变、可分区、支持并行计算的数据集合。</p>
<blockquote>
<p>《Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing》</p>
</blockquote>
<p>RDD特性：</p>
<ul>
<li>弹性<ul>
<li>存储：内存不足时可以和磁盘进行数据交换</li>
<li>计算：计算出错时支持重试</li>
<li>容错：数据丢失可以自动恢复</li>
<li>分片：支持重新分片</li>
</ul>
</li>
<li>不可变：RDD只读，只能通过transformer生成新的RDD</li>
<li>支持并行计算：不同分区可以调度到不同节点上进行计算。</li>
</ul>
<p>五个核心方法：</p>
<table>
<thead>
<tr>
<th><strong>名称</strong></th>
<th><strong>含义</strong></th>
</tr>
</thead>
<tbody><tr>
<td>getPartitions</td>
<td>由子类实现，返回一个分区列表，用于执行并行计算</td>
</tr>
<tr>
<td>compute</td>
<td>由子类实现，是一个用于 <strong>计算每一个分区</strong> 的 <strong>函数</strong></td>
</tr>
<tr>
<td>getDependencies</td>
<td>由子类实现，获取当前 RDD 的依赖关系</td>
</tr>
<tr>
<td>partitioner</td>
<td>由子类实现（可选），可设置分区器对数据集进行分区（仅适用于 KV 类型的 RDD）</td>
</tr>
<tr>
<td>getPreferredLocations</td>
<td>由子类实现（可选），可在分区计算时指定 <strong>优先起始位置</strong>，有助于“移动计算”的实现</td>
</tr>
</tbody></table>
<div class="code-wrapper"><pre><code class="hljs scala"># 创建<span class="hljs-type">RDD</span>
<span class="hljs-keyword">val</span> array: <span class="hljs-type">Array</span>[<span class="hljs-type">Int</span>] = <span class="hljs-type">Array</span>(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>)  
<span class="hljs-keyword">val</span> rdd: <span class="hljs-type">RDD</span>[<span class="hljs-type">Int</span>] = sc.parallelize(array)

# 文件读取
<span class="hljs-keyword">val</span> lines: <span class="hljs-type">RDD</span>[<span class="hljs-type">String</span>] = sc.textFile(<span class="hljs-string">&quot;data/my.txt&quot;</span>)</code></pre></div>


<h3 id="RDD-Partition"><a href="#RDD-Partition" class="headerlink" title="RDD Partition"></a>RDD Partition</h3><p>RDD Partition是数据源的部分片段，由InputFormat实现类按一定规则切分数据集之后的结果</p>
<h3 id="RDD算子"><a href="#RDD算子" class="headerlink" title="RDD算子"></a>RDD算子</h3><h4 id="转换算子-transformations"><a href="#转换算子-transformations" class="headerlink" title="转换算子(transformations)"></a>转换算子(transformations)</h4><p>转换算子会基于已有的RDD，按照一定规则创建新的RDD，仅记录RDD转换逻辑，不会触发计算。</p>
<table>
<thead>
<tr>
<th><strong>操作</strong></th>
<th><strong>含义</strong></th>
</tr>
</thead>
<tbody><tr>
<td><strong>filter</strong>(<em>func</em>)</td>
<td>筛选出满足条件的元素，并返回一个新的数据集</td>
</tr>
<tr>
<td><strong>map</strong>(<em>func</em>)</td>
<td>将每个元素传递到函数 <em>func</em> 中，返回一个新的数据集，每个输入元素会映射到 <strong>1 个输出结果</strong></td>
</tr>
<tr>
<td><strong>flatMap</strong>(<em>func</em>)</td>
<td>与 map 相似，但每个输入元素都可以映射到 <strong>0 或多个输出结果</strong></td>
</tr>
<tr>
<td><strong>mapPartitions</strong>(<em>func</em>)</td>
<td>与 map 相似，但是传递给函数 <em>func</em> 的是每个分区数据集对应的迭代器</td>
</tr>
<tr>
<td><strong>distinct</strong>(<em>func</em>)</td>
<td>对原数据集进行去重，并返回新的数据集</td>
</tr>
<tr>
<td><strong>groupByKey</strong>(<em>[numPartitions]</em>)</td>
<td>应用于 (K, V) 形式的数据集，返回一个新的 (K, Iterable<V>) 形式的数据集，可通过 <em>numPartitions</em> 指定新数据集的分区数</td>
</tr>
<tr>
<td><strong>reduceByKey</strong>(<em>func</em>, <em>[numPartitions]</em>)</td>
<td>应用于 (K, V) 形式的数据集，返回一个新的 (K, V) 形式的数据集，新数据集中的 V 是原有数据集中每个 K 对应的 V 传递到 <em>func</em> 中进行聚合后的结果</td>
</tr>
<tr>
<td><strong>aggregateByKey</strong>(<em>zeroValue</em>)(<em>seqOp</em>, <em>combOp</em>, <em>[numPartitions]</em>)</td>
<td>应用于 (K, V) 形式的数据集，返回一个新的 (K, U) 形式的数据集，新数据集中的 U 是原有数据集中每个 K 对应的 V 传递到 <em>seqOp</em> 与 <em>combOp</em> 的联合函数且与 <em>zeroValue</em> 聚合后的结果</td>
</tr>
<tr>
<td><strong>sortByKey</strong>(<em>[ascending]</em>, <em>[numPartitions]</em>)</td>
<td>应用于 (K, V) 形式的数据集，返回一个根据 K 排序的数据集，K 按升序或降序排序由 <em>ascending</em> 指定</td>
</tr>
<tr>
<td><strong>union</strong>(<em>func</em>)</td>
<td>将两个数据集中的元素合并到一个新的数据集</td>
</tr>
<tr>
<td><strong>join</strong>(<em>func</em>)</td>
<td>表示内连接，对于给定的两个形式分别为 (K, V) 和 (K, W) 的数据集，只有在两个数据集中都存在的 K 才会被输出，最终得到一个 (K, (V, W)) 类型的数据集</td>
</tr>
<tr>
<td><strong>repartition</strong>(<em>numPartitions</em>)</td>
<td>对数据集进行重分区，新的分区数由 <em>numPartitions</em> 指定，包含shuffle操作，扩大&#x2F;缩小分区都可用，性能较coaleace差</td>
</tr>
<tr>
<td><strong>coaleace</strong>(<em>numPartitions</em>)</td>
<td>对数据集进行重分区，新的分区数由 <em>numPartitions</em> 指定，不包含shuffle操作，缩小分区可用coaleace</td>
</tr>
</tbody></table>
<h4 id="行动算子-actions"><a href="#行动算子-actions" class="headerlink" title="行动算子(actions)"></a>行动算子(actions)</h4><p>不会返回新的RDD，但是<strong>会触发任务执行</strong>，算子执行后，会向Spark发起job，Spark按照前置转换算子生成DAG并执行。</p>
<table>
<thead>
<tr>
<th><strong>操作</strong></th>
<th><strong>含义</strong></th>
</tr>
</thead>
<tbody><tr>
<td><strong>count</strong>()</td>
<td>返回数据集中的元素个数</td>
</tr>
<tr>
<td><strong>countByKey</strong>()</td>
<td>仅适用于 (K, V) 形式的数据集，以 (K, Int) 形式的 Map 返回每个 K 的元素个数</td>
</tr>
<tr>
<td><strong>collect</strong>()</td>
<td>以数组的形式返回数据集中的所有元素</td>
</tr>
<tr>
<td><strong>first</strong>()</td>
<td>返回数据集中的第一个元素</td>
</tr>
<tr>
<td><strong>take</strong>(<em>n</em>)</td>
<td>以数组的形式返回数据集中的前 <em>n</em> 个元素</td>
</tr>
<tr>
<td><strong>reduce</strong>(<em>func</em>)</td>
<td>通过函数 _func_（输入两个参数并返回一个值）聚合数据集中的元素</td>
</tr>
<tr>
<td><strong>foreach</strong>(<em>func</em>)</td>
<td>将数据集中的每个元素传递到函数 <em>func</em> 中运行</td>
</tr>
<tr>
<td><strong>saveAsTextFile</strong>(<em>path</em>)</td>
<td>将数据集以文本格式写到本地磁盘或 HDFS 的指定目录下</td>
</tr>
<tr>
<td><strong>saveAsSequenceFile</strong>(<em>path</em>)</td>
<td>将数据集以 SequenceFile 格式写到本地磁盘或 HDFS 的指定目录下，仅适用于 (K, V) 形式且 K 和 V 均实现了 Hadoop Writable 接口的数据集</td>
</tr>
<tr>
<td><strong>saveAsObjectFile</strong>(<em>path</em>)</td>
<td>将数据集序列化成对象保存至本地磁盘或 HDFS 的指定目录下</td>
</tr>
</tbody></table>
<h3 id="RDD依赖关系"><a href="#RDD依赖关系" class="headerlink" title="RDD依赖关系"></a>RDD依赖关系</h3><p>转换算子生成的新RDD，与原始RDD存在依赖关系，代码层面使用<strong>Dependency</strong>关联。</p>
<p>依赖类型：</p>
<ul>
<li>宽依赖：父RDD每个分区对应子RDD的多个分区，通常存在于groupByKey、reduceByKey等操作，需要对RDD分区做shuffle。<ul>
<li><strong>ShuffleDependency</strong></li>
</ul>
</li>
<li>窄依赖：父RDD的每个分区，最多被子RDD的一个分区使用。通常存在于map、filter、union等操作，<strong>一个输入分区对应一个输出分区</strong>。<ul>
<li><strong>OneToOneDependency</strong>、<strong>RangeDependency</strong></li>
</ul>
</li>
</ul>
<p>部分RDD数据丢失时，可以通过依赖关系重新计算，进而恢复丢失数据的分区。</p>
<h3 id="RDD持久化"><a href="#RDD持久化" class="headerlink" title="RDD持久化"></a>RDD持久化</h3><p>Spark会将RDD持久化到磁盘，当actions算子需要使用时，从磁盘读取，避免重新计算。</p>
<p>可以使用<code>persist()</code>方法来指定持久化。</p>
<table>
<thead>
<tr>
<th><strong>持久化级别</strong></th>
<th><strong>含义</strong></th>
</tr>
</thead>
<tbody><tr>
<td>MEMORY_ONLY</td>
<td>将 RDD 以反序列化 Java 对象的形式存储在 JVM 中，如果大小超过可用内存，则超出部分不会缓存，需重新计算</td>
</tr>
<tr>
<td>MEMORY_AND_DISK</td>
<td>将 RDD 以反序列化 Java 对象的形式存储在 JVM 中，如果大小超过可用内存，则超出部分会存在在磁盘上，当需要时从磁盘读取</td>
</tr>
<tr>
<td>DISK_ONLY</td>
<td>将所有 RDD 分区存储到磁盘上</td>
</tr>
<tr>
<td>MEMORY_ONLY_SER</td>
<td>将 RDD 以序列化 Java 对象的形式存储在 JVM 中，具有更好的空间利用率，但是需要占用更多的 CPU 资源</td>
</tr>
<tr>
<td>MEMORY_AND_DISK_SER</td>
<td>将 RDD 以序列化 Java 对象的形式存储在 JVM 中，如果大小超过可用内存，则超出部分会存在在磁盘上，无需重新计算</td>
</tr>
<tr>
<td>MEMORY_ONLY_2</td>
<td>与 MEMORY_ONLY 级别相同，存在副本</td>
</tr>
<tr>
<td>MEMORY_AND_DISK_2</td>
<td>与 MEMORY_AND_DISK 级别相同，存在副本</td>
</tr>
</tbody></table>
<h3 id="RDD-Checkpoint"><a href="#RDD-Checkpoint" class="headerlink" title="RDD Checkpoint"></a>RDD Checkpoint</h3><p>RDD Checkpoint是一种容错保障机制，由<code>checkpoint()</code>触发，主要执行：</p>
<ol>
<li>重新计算调用了<code>checkpoint()</code>的RDD，并将结果保存到存储系统，可以通过<code>sc.setCheckpointDir(&quot;checkpoint&quot;)</code>修改存储地址</li>
<li>切断原有的依赖血缘关系</li>
</ol>
<p>与持久化有所区别：</p>
<table>
<thead>
<tr>
<th>区别项</th>
<th>RDD 持久化</th>
<th>RDD 检查点</th>
</tr>
</thead>
<tbody><tr>
<td>生命周期</td>
<td>应用结束便删除</td>
<td>永久保存</td>
</tr>
<tr>
<td>血缘关系</td>
<td>不切断</td>
<td>切断</td>
</tr>
<tr>
<td>使用场景</td>
<td>支持在同一个应用中复用计算结果</td>
<td>支持在多个应用中复用计算结果</td>
</tr>
</tbody></table>
<p>checkpoint在任务执行结束后触发：</p>
<div class="code-wrapper"><pre><code class="hljs scala"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">runJob</span></span>[<span class="hljs-type">T</span>, <span class="hljs-type">U</span>: <span class="hljs-type">ClassTag</span>](  
rdd: <span class="hljs-type">RDD</span>[<span class="hljs-type">T</span>],  
func: (<span class="hljs-type">TaskContext</span>, <span class="hljs-type">Iterator</span>[<span class="hljs-type">T</span>]) =&gt; <span class="hljs-type">U</span>,  
partitions: <span class="hljs-type">Seq</span>[<span class="hljs-type">Int</span>],  
resultHandler: (<span class="hljs-type">Int</span>, <span class="hljs-type">U</span>) =&gt; <span class="hljs-type">Unit</span>): <span class="hljs-type">Unit</span> = &#123;  
<span class="hljs-comment">// ...  </span>
dagScheduler.runJob(rdd, cleanedFunc, partitions, callSite, resultHandler, localProperties.get)  
progressBar.foreach(_.finishAll())  
**rdd.doCheckpoint()**  
&#125;</code></pre></div>


<h2 id="共享变量"><a href="#共享变量" class="headerlink" title="共享变量"></a>共享变量</h2><div class="code-wrapper"><pre><code class="hljs scala"><span class="hljs-keyword">val</span> rdd: <span class="hljs-type">RDD</span>[<span class="hljs-type">Int</span>] = sc.makeRDD(<span class="hljs-type">List</span>(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>))  
  
<span class="hljs-keyword">var</span> sum: <span class="hljs-type">Int</span> = <span class="hljs-number">0</span>  
rdd.foreach(num =&gt; &#123;  
sum += num  
&#125;)  
  
println(<span class="hljs-string">&quot;sum =&gt; &quot;</span> + sum)</code></pre></div>

<p>上述代码提交到Spark执行，sum最终结果为0，这是因为计算逻辑是分发到Executor执行的，Executor将计算结果返回给Driver时，不会将非RDD内部数据的普通变量返回，也就是<code>sum</code>并没有参与计算。</p>
<p>为了解决上述问题，Spark引入了累加器和广播变量。</p>
<h3 id="累加器（Accumulators）"><a href="#累加器（Accumulators）" class="headerlink" title="累加器（Accumulators）"></a>累加器（Accumulators）</h3><div class="code-wrapper"><pre><code class="hljs scala"><span class="hljs-keyword">val</span> rdd: <span class="hljs-type">RDD</span>[<span class="hljs-type">Int</span>] = sc.makeRDD(<span class="hljs-type">List</span>(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>))  
  
<span class="hljs-keyword">val</span> sumAccumulator: <span class="hljs-type">LongAccumulator</span> = sc.longAccumulator(<span class="hljs-string">&quot;sum&quot;</span>)  
rdd.foreach(num =&gt; &#123;  
sumAccumulator.add(num)  
&#125;)  
  
println(<span class="hljs-string">&quot;sum =&gt; &quot;</span> + sumAccumulator.value)</code></pre></div>


<h3 id="广播变量"><a href="#广播变量" class="headerlink" title="广播变量"></a>广播变量</h3><p>多个Task需要共享同一个大型变量时，可以使用广播变量优化。广播变量会通过Driver分发到每一个Executor中。</p>
<div class="code-wrapper"><pre><code class="hljs scala"><span class="hljs-comment">// 创建单词列表list </span>
<span class="hljs-keyword">val</span> list: <span class="hljs-type">List</span>[<span class="hljs-type">String</span>] = <span class="hljs-type">List</span>(<span class="hljs-string">&quot;Apache&quot;</span>, <span class="hljs-string">&quot;Spark&quot;</span>) 
<span class="hljs-comment">// 创建广播变量bc </span>
<span class="hljs-keyword">val</span> bc = sc.broadcast(list)</code></pre></div>


<h2 id="Spark调度模块"><a href="#Spark调度模块" class="headerlink" title="Spark调度模块"></a>Spark调度模块</h2><table>
<thead>
<tr>
<th>关键步骤</th>
<th>所在进程</th>
<th>核心组件</th>
</tr>
</thead>
<tbody><tr>
<td>将DAG拆分为不同的Stages，根据Stages创建分布式任务Tasks和任务组TaskSets</td>
<td>Driver</td>
<td>DAGScheduler</td>
</tr>
<tr>
<td>获取集群内可用计算资源</td>
<td>Driver</td>
<td>SchedulerBackend</td>
</tr>
<tr>
<td>根据调度规则决定任务优先级，完成任务调度</td>
<td>Driver</td>
<td>TaskScheduler</td>
</tr>
<tr>
<td>依次将分布式任务发送到Executors</td>
<td>Driver</td>
<td>SchedulerBackend</td>
</tr>
<tr>
<td>执行收到的分布式任务</td>
<td>Executors</td>
<td>ExecutorBackend</td>
</tr>
</tbody></table>
<h2 id="内存管理"><a href="#内存管理" class="headerlink" title="内存管理"></a>内存管理</h2><p>Spark Executor会将内存划分成四个区域：</p>
<ul>
<li>Reserved Memory：固定为300MB，Spark预留空间，用于存储Spark内部对象，</li>
<li>User Memory：存储程序自定义的数据对象。</li>
<li>Execution Memory：用于执行任务计算，包括数据转换、过滤、排序、聚合等。</li>
<li>Storage Memory：用于缓存分布式数据集，比如RDD Cache（持久化内存），广播变量等。</li>
</ul>
<p>Execution Memory和Storage Memory可以相互抢占空间（对方内存有空闲即可），若被抢占方有内存需求时，需要归还。Execution Memory抢占的空间需要等分布式任务执行完毕后才能归还。</p>
<h3 id="内存配置项"><a href="#内存配置项" class="headerlink" title="内存配置项"></a>内存配置项</h3><p><img src="https://blog-1251613845.cos.ap-shanghai.myqcloud.com/20240912195907.png" srcset="/img/loading.gif" lazyload alt="image.png"></p>
<ul>
<li>spark.executor.memory（绝对值）：指定了Executor进程的JVM堆内存大小</li>
<li>spark.memory.fraction（比例）：Execution Memory和Storage Memory两部分区域</li>
<li>spark.memory.storageFraction（比例）：区分Execution Memory和Storage Memory的初始大小</li>
</ul>
<h2 id="Spark-Streaming"><a href="#Spark-Streaming" class="headerlink" title="Spark Streaming"></a>Spark Streaming</h2><p>Spark Streaming是基于Spark Core实现的流式计算框架。</p>
<p>从实现上来看，并不是真正的流式计算，而是将数据包装成一批数据（Mirco Batch），从而达到类似流式计算的效果。</p>
<p><img src="https://blog-1251613845.cos.ap-shanghai.myqcloud.com/20240731141319.png" srcset="/img/loading.gif" lazyload alt="image.png"></p>
<div class="code-wrapper"><pre><code class="hljs scala"><span class="hljs-class"><span class="hljs-keyword">object</span> <span class="hljs-title">WordCountStream</span> </span>&#123;  
  
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span></span>(args: <span class="hljs-type">Array</span>[<span class="hljs-type">String</span>]): <span class="hljs-type">Unit</span> = &#123;  
<span class="hljs-comment">// 1. 设置运行模式为 local，线程数为 3  </span>
<span class="hljs-keyword">val</span> conf: <span class="hljs-type">SparkConf</span> = <span class="hljs-keyword">new</span> <span class="hljs-type">SparkConf</span>().setMaster(<span class="hljs-string">&quot;local[3]&quot;</span>).setAppName(<span class="hljs-string">&quot;Word Count Streaming App&quot;</span>)  
<span class="hljs-comment">// 2. 创建批次周期为 10s 的 StreamingContext  </span>
<span class="hljs-keyword">val</span> ssc: <span class="hljs-type">StreamingContext</span> = <span class="hljs-keyword">new</span> <span class="hljs-type">StreamingContext</span>(conf, <span class="hljs-type">Seconds</span>(<span class="hljs-number">10</span>))  
  
<span class="hljs-comment">// 3. 创建来自于本地 9999 端口的 Dtream 对象  </span>
<span class="hljs-keyword">val</span> inputStream: <span class="hljs-type">ReceiverInputDStream</span>[<span class="hljs-type">String</span>] = ssc.socketTextStream(<span class="hljs-string">&quot;localhost&quot;</span>, <span class="hljs-number">9999</span>)  
  
<span class="hljs-comment">// 4. 定义 DStream 转换算子  </span>
<span class="hljs-keyword">val</span> wcStream: <span class="hljs-type">DStream</span>[(<span class="hljs-type">String</span>, <span class="hljs-type">Int</span>)] = inputStream  
.flatMap(_.split(<span class="hljs-string">&quot; &quot;</span>))  
.map(word =&gt; (word, <span class="hljs-number">1</span>))  
.reduceByKey(_ + _)  
  
<span class="hljs-comment">// 5. 定义 DStream 输出算子  </span>
wcStream.print()  
  
<span class="hljs-comment">// 6. 启动计算作业  </span>
ssc.start()  
<span class="hljs-comment">// 7. 进行阻塞，防止 Driver 进程退出  </span>
ssc.awaitTermination()  
&#125;  
&#125;</code></pre></div>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/Big-Data/" class="category-chain-item">Big Data</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>Spark入门</div>
      <div>https://l1n.wang/2024/Big-Data/spark-note/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>Lin Wang</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2024年7月12日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-cc-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2024/Big-Data/flink-note/" title="Flink入门">
                        <span class="hidden-mobile">Flink入门</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
  
  
    <article id="comments" lazyload>
      
  <script type="text/javascript">
    Fluid.utils.loadComments('#comments', function() {
      var light = 'github-light';
      var dark = 'github-dark';
      var schema = document.documentElement.getAttribute('data-user-color-scheme');
      if (schema === 'dark') {
        schema = dark;
      } else {
        schema = light;
      }
      window.UtterancesThemeLight = light;
      window.UtterancesThemeDark = dark;
      var s = document.createElement('script');
      s.setAttribute('src', 'https://utteranc.es/client.js');
      s.setAttribute('repo', 'L1nker4/blog_comment');
      s.setAttribute('issue-term', 'title');
      
      s.setAttribute('label', 'utterances');
      
      s.setAttribute('theme', schema);
      s.setAttribute('crossorigin', 'anonymous');
      document.getElementById('comments').appendChild(s);
    })
  </script>
  <noscript>Please enable JavaScript to view the comments</noscript>


    </article>
  


          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  


  
  









    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       2018 - 2024 
    </div>
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://cdn.jsdelivr.net/npm/nprogress@0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js" ></script>
<script  src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://cdn.jsdelivr.net/npm/typed.js@2/lib/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://cdn.jsdelivr.net/npm/tocbot@4/dist/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://cdn.jsdelivr.net/npm/anchor-js@4/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  
      <script>
        if (!window.MathJax) {
          window.MathJax = {
            tex    : {
              inlineMath: { '[+]': [['$', '$']] }
            },
            loader : {
              load: ['ui/lazy']
            },
            options: {
              renderActions: {
                insertedScript: [200, () => {
                  document.querySelectorAll('mjx-container').forEach(node => {
                    let target = node.parentNode;
                    if (target.nodeName.toLowerCase() === 'li') {
                      target.parentNode.classList.add('has-jax');
                    }
                  });
                }, '', false]
              }
            }
          };
        } else {
          MathJax.startup.document.state(0);
          MathJax.texReset();
          MathJax.typeset();
          MathJax.typesetPromise();
        }

        Fluid.events.registerRefreshCallback(function() {
          if ('MathJax' in window && MathJax.startup.document && typeof MathJax.startup.document.state === 'function') {
            MathJax.startup.document.state(0);
            MathJax.texReset();
            MathJax.typeset();
            MathJax.typesetPromise();
          }
        });
      </script>
    

  <script  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" ></script>

  <script  src="/js/local-search.js" ></script>




  
<script src="//cdn.jsdelivr.net/gh/EmoryHuang/BlogBeautify@1.1/love.min.js"></script>



<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
